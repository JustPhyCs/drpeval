{
 "metadata": {
  "name": "",
  "signature": "sha256:ab43ced3eed7d28571f98a3c1d2169ecd473c5222b26a73a71ddcd7ff73edcc5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pywt\n",
      "import numpy as np\n",
      "import h5py as h5\n",
      "import subprocess\n",
      "import os\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#run_num_list=['182','184','185','186','188','190','191','192','193','194','196','197']\n",
      "\n",
      "run_num_list=['182']\n",
      "\n",
      "\n",
      "## In the index_information variable, the [:,0] is global index,\n",
      "##[:,1] is local index, [:,2] is file index\n",
      "\n",
      "for num in range(len(run_num_list)):\n",
      "    \n",
      "    run_num=run_num_list[num]\n",
      "        \n",
      "    ## If there are images with single hit, then load the h5 file and do data compression\n",
      "    path='/reg/d/psdm/amo/amo86615/res/yoon82/data/'\n",
      "    file_name = 'amo86615_'+run_num+'_PR772_single.h5'\n",
      "    \n",
      "    h5file=h5.File(path+file_name)\n",
      "    photon_count=h5file['photonConverter/pnccdBack/photonCount']\n",
      "    \n",
      "    \n",
      "    ## Do the data compression, reconstruction\n",
      "    for indx2 in range(photon_count.shape[0]):\n",
      "        ll,(LH,HL,HH)= pywt.dwt2(photon_count[indx2,::,::],'db2')\n",
      "        \n",
      "        zeros_pad = np.zeros_like(HL)\n",
      "        new_coefficient = (ll,(zeros_pad,zeros_pad,zeros_pad))\n",
      "        if indx2 ==0 :\n",
      "            re_photon_count= np.zeros((photon_count.shape[0],photon_count.shape[1],photon_count.shape[2]))\n",
      "            ll_photon_count= np.zeros((photon_count.shape[0],ll.shape[0],ll.shape[1]))\n",
      "        \n",
      "        ## re_photon_count contains the reconstructed images\n",
      "        ## cp_photon_count contains the low-low coefficient\n",
      "        re_photon_count[indx2,::,::]=pywt.idwt2(new_coefficient,'db2')[0:photon_count.shape[1],0:photon_count.shape[2]]\n",
      "        ll_photon_count[indx2,::,::]=ll\n",
      "        \n",
      "    ## copy two different copies _single.h5 file to my folder. one contains the reconstructed image\n",
      "    ## The other contains the compressed images.\n",
      "    path_new = '/reg/d/psdm/cxi/cxitut13/res/haoyuan/wavelet_compressed_data_1/'\n",
      "    file_ll = 'amo86615_'+run_num+'_PR772_single_ll.h5'\n",
      "    file_re = 'amo86615_'+run_num+'_PR772_single_re.h5'\n",
      "    os.system('rm %s%s'%(path_new,file_ll))\n",
      "    os.system('rm %s%s'%(path_new,file_re))\n",
      "    os.system('cp %s%s %s%s'%(path,file_name,path_new,file_ll))\n",
      "    os.system('cp %s%s %s%s'%(path,file_name,path_new,file_re))\n",
      "    \n",
      "    \n",
      "    ## edit the h5 ll files\n",
      "    h5ll=h5.File(path_new+file_ll,'r+')\n",
      "    del h5ll['photonConverter/pnccdBack/photonCount']\n",
      "    ll_data= h5ll.create_dataset('photonConverter/pnccdBack/photonCount',ll_photon_count.shape,'f',data = ll_photon_count)\n",
      "\n",
      "    ## Copy and change the attributes of the data set.\n",
      "    for attrs in photon_count.attrs.items():\n",
      "        ll_data.attrs.__setitem__(attrs[0],attrs[1])\n",
      "\n",
      "    ll_data.attrs.__setitem__('downsampleCols',ll_photon_count.shape[2])\n",
      "    ll_data.attrs.__setitem__('downsampleRows',ll_photon_count.shape[1])\n",
      "    ll_data.attrs.__setitem__('downsample',4)\n",
      "    ll_data.attrs.__setitem__('filepath',path_new+file_ll)\n",
      "    ll_data.attrs.__setitem__('wavelet','ll component, db2 family')\n",
      "\n",
      "    ## Save the change to the h5 file.\n",
      "    h5ll.flush()\n",
      "    h5ll.close()\n",
      "    \n",
      "\n",
      "    ## edit the h5 reconstruct files\n",
      "    h5re=h5.File(path_new+file_re,'r+')\n",
      "    del h5re['photonConverter/pnccdBack/photonCount']\n",
      "    re_data= h5re.create_dataset('photonConverter/pnccdBack/photonCount',re_photon_count.shape,'f',data = re_photon_count)\n",
      "\n",
      "    ## Copy and change the attributes of the data set.\n",
      "    for attrs in photon_count.attrs.items():\n",
      "        re_data.attrs.__setitem__(attrs[0],attrs[1])\n",
      "\n",
      "    re_data.attrs.__setitem__('downsampleCols',re_photon_count.shape[2])\n",
      "    re_data.attrs.__setitem__('downsampleRows',re_photon_count.shape[1])\n",
      "    re_data.attrs.__setitem__('downsample',4)\n",
      "    re_data.attrs.__setitem__('filepath',path_new+file_re)\n",
      "    re_data.attrs.__setitem__('wavelet','reconstructed from ll component, db2 family')\n",
      "\n",
      "    ## Save the change to the h5 file.\n",
      "    h5re.flush()\n",
      "    h5re.close()\n",
      "    \n",
      "    \n",
      "    ## Close the original h5 file.\n",
      "    h5file.close()\n",
      "    \n",
      "    print 'Finished %s' % run_num\n",
      "            \n",
      "        \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Finished 182\n",
        "Finished 184"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 185"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 186"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 188"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 190"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 192"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 193"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 194"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 196"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run_num_list=['182','183','184','185','186','188','190','191','192','193','194','196','197']\n",
      "\n",
      "#run_num_list=['182']\n",
      "\n",
      "## In the index_information variable, the [:,0] is global index,\n",
      "##[:,1] is local index, [:,2] is file index\n",
      "index_information = np.load('/reg/neh/home/haoyuan/sh_scripts_current/index_information.npy')\n",
      "\n",
      "for num in range(len(run_num_list)):\n",
      "    run_num=run_num_list[num]\n",
      "    \n",
      "    ## First get the local index information\n",
      "    local_index = []\n",
      "    for indx1 in range(index_information.shape[0]):\n",
      "        if index_information[indx1,2] == num:\n",
      "            local_index.append(index_information[indx1,1])\n",
      "    ## If there is no single hit image, continue to the next file\n",
      "    if len(local_index)<1 :\n",
      "        continue\n",
      "        \n",
      "    ## If there are images with single hit, then load the h5 file and do data compression\n",
      "    path='/reg/d/psdm/amo/amo86615/res/yoon82/data/'\n",
      "    file_name = 'amo86615_'+run_num+'_PR772_single.h5'\n",
      "    \n",
      "    h5file=h5.File(path+file_name)\n",
      "    photon_count=h5file['photonConverter/pnccdBack/photonCount']\n",
      "    \n",
      "    ## make a copy to make change to this file\n",
      "    raw_photon_count= photon_count[::,::,::]\n",
      "    ## zero padding the raw image data\n",
      "    zeros_pad=np.zeros((raw_photon_count.shape[0],raw_photon_count.shape[1],1))\n",
      "    raw_photon_count=np.append(raw_photon_count,zeros_pad,axis=2)\n",
      "    \n",
      "    \n",
      "    ## Do the data compression, reconstruction\n",
      "    for indx2 in range(len(local_index)):\n",
      "        ll,(LH,HL,HH)= pywt.dwt2(raw_photon_count[local_index[indx2],::,::],'db1')\n",
      "        \n",
      "        zeros_pad = np.zeros_like(HL)\n",
      "        new_coefficient = (ll,(zeros_pad,zeros_pad,zeros_pad))\n",
      "        if indx2 ==0 :\n",
      "            re_photon_count= np.zeros((len(local_index),photon_count.shape[1],photon_count.shape[2]))\n",
      "            ll_photon_count= np.zeros((len(local_index),ll.shape[0],ll.shape[1]))\n",
      "        \n",
      "        ## re_photon_count contains the reconstructed images\n",
      "        ## cp_photon_count contains the low-low coefficient\n",
      "        re_photon_count[indx2,::,::]=pywt.idwt2(new_coefficient,'db1')[0:photon_count.shape[1],0:photon_count.shape[2]]\n",
      "        ll_photon_count[indx2,::,::]=ll\n",
      "        \n",
      "    re_photon_count=re_photon_count.astype('uint16')\n",
      "    ll_photon_count=ll_photon_count.astype('uint16')\n",
      "    ## copy two different copies _single.h5 file to my folder. one contains the reconstructed image\n",
      "    ## The other contains the compressed images.\n",
      "    path_new = '/reg/d/psdm/cxi/cxitut13/res/haoyuan/wavelet_compressed_data/'\n",
      "    file_ll = 'amo86615_'+run_num+'_PR772_single_LL.h5'\n",
      "    file_re = 'amo86615_'+run_num+'_PR772_single_re.h5'\n",
      "    os.system('rm %s%s'%(path_new,file_ll))\n",
      "    os.system('rm %s%s'%(path_new,file_re))\n",
      "    os.system('cp %s%s %s%s'%(path,file_name,path_new,file_ll))\n",
      "    os.system('cp %s%s %s%s'%(path,file_name,path_new,file_re))\n",
      "    \n",
      "    \n",
      "    ## edit the h5 ll files\n",
      "    h5ll=h5.File(path_new+file_ll,'r+')\n",
      "    h5ll.__delitem__('photonConverter/pnccdBack/photonCount')\n",
      "    ll_data= h5ll.create_dataset('photonConverter/pnccdBack/photonCount',\n",
      "                                             ll_photon_count.shape,'uint16',data = ll_photon_count)\n",
      "\n",
      "    ## Copy and change the attributes of the data set.\n",
      "    for attrs in photon_count.attrs.items():\n",
      "        ll_data.attrs.__setitem__(attrs[0],attrs[1])\n",
      "\n",
      "    ll_data.attrs.__setitem__('downsampleCols',ll_photon_count.shape[2])\n",
      "    ll_data.attrs.__setitem__('downsampleRows',ll_photon_count.shape[1])\n",
      "    ll_data.attrs.__setitem__('downsample',4)\n",
      "    ll_data.attrs.__setitem__('filepath',path_new+file_ll)\n",
      "    ll_data.attrs.__setitem__('wavelet','ll component, db1 family')\n",
      "\n",
      "    ## Save the change to the h5 file.\n",
      "#    h5ll.flush()\n",
      "#    h5ll.close()\n",
      "    \n",
      "\n",
      "    ## edit the h5 reconstruct files\n",
      "    h5re=h5.File(path_new+file_re,'r+')\n",
      "    h5re.__delitem__('photonConverter/pnccdBack/photonCount')\n",
      "    re_data= h5re.create_dataset('photonConverter/pnccdBack/photonCount',\n",
      "                                             re_photon_count.shape,'uint16',data = re_photon_count)\n",
      "\n",
      "    ## Copy and change the attributes of the data set.\n",
      "    for attrs in photon_count.attrs.items():\n",
      "        re_data.attrs.__setitem__(attrs[0],attrs[1])\n",
      "\n",
      "    re_data.attrs.__setitem__('downsampleCols',re_photon_count.shape[2])\n",
      "    re_data.attrs.__setitem__('downsampleRows',re_photon_count.shape[1])\n",
      "    re_data.attrs.__setitem__('downsample',4)\n",
      "    re_data.attrs.__setitem__('filepath',path_new+file_re)\n",
      "    re_data.attrs.__setitem__('wavelet','reconstructed from ll component, db family')\n",
      "\n",
      "    ## Save the change to the h5 file.\n",
      "#    h5re.flush()\n",
      "#    h5re.close()\n",
      "    \n",
      "    \n",
      "    ## Close the original h5 file.\n",
      "#    h5file.close()\n",
      "    \n",
      "    print 'Finished %s' % run_num\n",
      "            \n",
      "        \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Finished 182\n",
        "Finished 184"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 185"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 186"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 188"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 190"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 191"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 192"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 193"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 194"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 196"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Finished 197"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw_photon_count.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "(15, 260, 258)"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.fromfile('/reg/d/psdm/cxi/cxitut13/res/haoyuan/emc_tests/factor4_v2//aux/mask3')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "(6, 130, 129)"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}